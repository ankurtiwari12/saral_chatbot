{
  "default": [
    {
      "bullet_id": "bullet_0",
      "bullet_text": "• **Attention Function**: The attention function takes a query (Q), key (K), and value (V) as input and outputs a weight vector, which represents the importance of each value for the query. This is achieved through a softmax function applied to the dot product of Q and K.",
      "source_chunk": "Scaled Dot-Product Attention\n Multi-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are also packed together into matrices Kand V. We compute\nthe matrix of outputs as:\nAttention(Q,K,V ) = softmax(QKT\n√dk\n)V (1)\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\nof 1√dk",
      "source_metadata": {
        "total_pages": 11,
        "producer": "PyPDF2",
        "publisher": "Curran Associates, Inc.",
        "subject": "Neural Information Processing Systems http://nips.cc/",
        "date": "2017",
        "equation_count": 0,
        "math_symbols": "",
        "moddate": "2018-02-12T21:22:10-08:00",
        "published": "2017",
        "canonical_forms": "",
        "sympy_parsed": false,
        "page": 3,
        "lastpage": "6008",
        "created": "2017",
        "firstpage": "5998",
        "language": "en-US",
        "creationdate": "",
        "page_label": "4",
        "editors": "I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett",
        "source": "/home/ankur/Downloads/iiit/rag/pdfs/NIPS-2017-attention-is-all-you-need-Paper.pdf",
        "id": "/home/ankur/Downloads/iiit/rag/pdfs/NIPS-2017-attention-is-all-you-need-Paper.pdf:3:0",
        "eventtype": "Poster",
        "book": "Advances in Neural Information Processing Systems 30",
        "author": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin",
        "title": "Attention is All you Need",
        "description-abstract": "The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.",
        "description": "Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)",
        "equation_ids": "",
        "has_math": false,
        "math_density": 0.0,
        "creator": "PyPDF",
        "type": "Conference Proceedings"
      },
      "feedback": "rejected",
      "timestamp": "2025-11-18T23:15:38.207607"
    },
    {
      "bullet_id": "bullet_1",
      "bullet_text": "**Scaled Dot-Product Attention**: This is a type of attention mechanism that scales the dot product by taking the square root of the dimensionality (dk) of the keys. The formula is: Attention(Q, K, V) = softmax(Q*K/√dk * V).",
      "source_chunk": "Scaled Dot-Product Attention\n Multi-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are also packed together into matrices Kand V. We compute\nthe matrix of outputs as:\nAttention(Q,K,V ) = softmax(QKT\n√dk\n)V (1)\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\nof 1√dk",
      "source_metadata": {
        "total_pages": 11,
        "producer": "PyPDF2",
        "publisher": "Curran Associates, Inc.",
        "subject": "Neural Information Processing Systems http://nips.cc/",
        "date": "2017",
        "equation_count": 0,
        "math_symbols": "",
        "moddate": "2018-02-12T21:22:10-08:00",
        "published": "2017",
        "canonical_forms": "",
        "sympy_parsed": false,
        "page": 3,
        "lastpage": "6008",
        "created": "2017",
        "firstpage": "5998",
        "language": "en-US",
        "creationdate": "",
        "page_label": "4",
        "editors": "I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett",
        "source": "/home/ankur/Downloads/iiit/rag/pdfs/NIPS-2017-attention-is-all-you-need-Paper.pdf",
        "id": "/home/ankur/Downloads/iiit/rag/pdfs/NIPS-2017-attention-is-all-you-need-Paper.pdf:3:0",
        "eventtype": "Poster",
        "book": "Advances in Neural Information Processing Systems 30",
        "author": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin",
        "title": "Attention is All you Need",
        "description-abstract": "The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.",
        "description": "Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)",
        "equation_ids": "",
        "has_math": false,
        "math_density": 0.0,
        "creator": "PyPDF",
        "type": "Conference Proceedings"
      },
      "feedback": "accepted",
      "timestamp": "2025-11-18T23:15:41.880499"
    },
    {
      "bullet_id": "bullet_2",
      "bullet_text": "**Multi-Head Attention**: Instead of performing a single attention function, multiple attention functions are run in parallel, each with a different set of projections. This allows the model to jointly attend to information from different representation subspaces.",
      "source_chunk": ".\n3.2.2 Multi-Head Attention\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\nwe found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\noutput values. These are concatenated and once again projected, resulting in the ﬁnal values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.",
      "source_metadata": {
        "creator": "PyPDF",
        "created": "2017",
        "eventtype": "Poster",
        "published": "2017",
        "book": "Advances in Neural Information Processing Systems 30",
        "math_symbols": "",
        "type": "Conference Proceedings",
        "total_pages": 11,
        "editors": "I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett",
        "moddate": "2018-02-12T21:22:10-08:00",
        "date": "2017",
        "subject": "Neural Information Processing Systems http://nips.cc/",
        "lastpage": "6008",
        "firstpage": "5998",
        "title": "Attention is All you Need",
        "description-abstract": "The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.",
        "page_label": "4",
        "author": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin",
        "math_density": 0.0,
        "id": "/home/ankur/Downloads/iiit/rag/pdfs/NIPS-2017-attention-is-all-you-need-Paper.pdf:3:2",
        "publisher": "Curran Associates, Inc.",
        "producer": "PyPDF2",
        "language": "en-US",
        "description": "Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)",
        "has_math": false,
        "creationdate": "",
        "canonical_forms": "",
        "equation_ids": "",
        "equation_count": 0,
        "sympy_parsed": false,
        "page": 3,
        "source": "/home/ankur/Downloads/iiit/rag/pdfs/NIPS-2017-attention-is-all-you-need-Paper.pdf"
      },
      "feedback": "accepted",
      "timestamp": "2025-11-18T23:15:52.140467"
    },
    {
      "bullet_id": "bullet_3",
      "bullet_text": "• **Parallel Attention Layers**: In multi-head attention, multiple attention layers (h) are run in parallel, each with a different set of projections. The output of each layer is concatenated and then projected again.",
      "source_chunk": ".\n3.2.2 Multi-Head Attention\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\nwe found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\noutput values. These are concatenated and once again projected, resulting in the ﬁnal values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.",
      "source_metadata": {
        "creator": "PyPDF",
        "created": "2017",
        "eventtype": "Poster",
        "published": "2017",
        "book": "Advances in Neural Information Processing Systems 30",
        "math_symbols": "",
        "type": "Conference Proceedings",
        "total_pages": 11,
        "editors": "I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett",
        "moddate": "2018-02-12T21:22:10-08:00",
        "date": "2017",
        "subject": "Neural Information Processing Systems http://nips.cc/",
        "lastpage": "6008",
        "firstpage": "5998",
        "title": "Attention is All you Need",
        "description-abstract": "The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.",
        "page_label": "4",
        "author": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin",
        "math_density": 0.0,
        "id": "/home/ankur/Downloads/iiit/rag/pdfs/NIPS-2017-attention-is-all-you-need-Paper.pdf:3:2",
        "publisher": "Curran Associates, Inc.",
        "producer": "PyPDF2",
        "language": "en-US",
        "description": "Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)",
        "has_math": false,
        "creationdate": "",
        "canonical_forms": "",
        "equation_ids": "",
        "equation_count": 0,
        "sympy_parsed": false,
        "page": 3,
        "source": "/home/ankur/Downloads/iiit/rag/pdfs/NIPS-2017-attention-is-all-you-need-Paper.pdf"
      },
      "feedback": "rejected",
      "timestamp": "2025-11-18T23:15:54.145351"
    },
    {
      "bullet_id": "bullet_4",
      "bullet_text": "**Projected Queries, Keys, and Values**: The queries, keys, and values are first projected using linear transformations (WQ, WK, WV). These projections result in lower-dimensional representations that can be processed more efficiently.",
      "source_chunk": ".\n3.2.2 Multi-Head Attention\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\nwe found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\noutput values. These are concatenated and once again projected, resulting in the ﬁnal values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.",
      "source_metadata": {
        "creator": "PyPDF",
        "created": "2017",
        "eventtype": "Poster",
        "published": "2017",
        "book": "Advances in Neural Information Processing Systems 30",
        "math_symbols": "",
        "type": "Conference Proceedings",
        "total_pages": 11,
        "editors": "I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett",
        "moddate": "2018-02-12T21:22:10-08:00",
        "date": "2017",
        "subject": "Neural Information Processing Systems http://nips.cc/",
        "lastpage": "6008",
        "firstpage": "5998",
        "title": "Attention is All you Need",
        "description-abstract": "The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.",
        "page_label": "4",
        "author": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin",
        "math_density": 0.0,
        "id": "/home/ankur/Downloads/iiit/rag/pdfs/NIPS-2017-attention-is-all-you-need-Paper.pdf:3:2",
        "publisher": "Curran Associates, Inc.",
        "producer": "PyPDF2",
        "language": "en-US",
        "description": "Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)",
        "has_math": false,
        "creationdate": "",
        "canonical_forms": "",
        "equation_ids": "",
        "equation_count": 0,
        "sympy_parsed": false,
        "page": 3,
        "source": "/home/ankur/Downloads/iiit/rag/pdfs/NIPS-2017-attention-is-all-you-need-Paper.pdf"
      },
      "feedback": "rejected",
      "timestamp": "2025-11-18T23:15:55.421820"
    },
    {
      "bullet_id": "bullet_6",
      "bullet_text": "• **Encoder-Decoder Attention**: In this application, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows the model to attend to information from different layers during decoding.",
      "source_chunk": "MultiHead(Q,K,V ) = Concat(head1,..., headh)WO\nwhere headi = Attention(QWQ\ni ,KW K\ni ,VW V\ni )\nWhere the projections are parameter matricesWQ\ni ∈Rdmodel×dk , WK\ni ∈Rdmodel×dk , WV\ni ∈Rdmodel×dv\nand WO ∈Rhdv×dmodel .\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\ndk = dv = dmodel/h= 64. Due to the reduced dimension of each head, the total computational cost\nis similar to that of single-head attention with full dimensionality.\n3.2.3 Applications of Attention in our Model\nThe Transformer uses multi-head attention in three different ways:\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\nand the memory keys and values come from the output of the encoder. This allows every",
      "source_metadata": {
        "editors": "I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett",
        "page_label": "5",
        "creator": "PyPDF",
        "total_pages": 11,
        "author": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin",
        "canonical_forms": "",
        "id": "/home/ankur/Downloads/iiit/rag/pdfs/NIPS-2017-attention-is-all-you-need-Paper.pdf:4:0",
        "creationdate": "",
        "math_symbols": "",
        "subject": "Neural Information Processing Systems http://nips.cc/",
        "book": "Advances in Neural Information Processing Systems 30",
        "equation_count": 0,
        "sympy_parsed": false,
        "description": "Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)",
        "firstpage": "5998",
        "type": "Conference Proceedings",
        "has_math": false,
        "source": "/home/ankur/Downloads/iiit/rag/pdfs/NIPS-2017-attention-is-all-you-need-Paper.pdf",
        "lastpage": "6008",
        "publisher": "Curran Associates, Inc.",
        "language": "en-US",
        "title": "Attention is All you Need",
        "created": "2017",
        "producer": "PyPDF2",
        "equation_ids": "",
        "page": 4,
        "moddate": "2018-02-12T21:22:10-08:00",
        "published": "2017",
        "date": "2017",
        "math_density": 0.0,
        "eventtype": "Poster",
        "description-abstract": "The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1."
      },
      "feedback": "accepted",
      "timestamp": "2025-11-18T23:15:57.973822"
    },
    {
      "bullet_id": "bullet_7",
      "bullet_text": "**Positional Encodings**: The positional encodings are used to represent the position of each token in the sequence. In this work, sine and cosine functions of different frequencies are employed for positional encodings.",
      "source_chunk": "Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\nLayer Type Complexity per Layer Sequential Maximum Path Length\nOperations\nSelf-Attention O(n2 ·d) O(1) O(1)\nRecurrent O(n·d2) O(n) O(n)\nConvolutional O(k·n·d2) O(1) O(logk(n))\nSelf-Attention (restricted) O(r·n·d) O(1) O(n/r)\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\nlearned and ﬁxed [8].\nIn this work, we use sine and cosine functions of different frequencies:",
      "source_metadata": {
        "author": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin",
        "date": "2017",
        "creationdate": "",
        "sympy_parsed": false,
        "firstpage": "5998",
        "total_pages": 11,
        "published": "2017",
        "producer": "PyPDF2",
        "created": "2017",
        "moddate": "2018-02-12T21:22:10-08:00",
        "language": "en-US",
        "subject": "Neural Information Processing Systems http://nips.cc/",
        "book": "Advances in Neural Information Processing Systems 30",
        "equation_count": 0,
        "page": 5,
        "equation_ids": "",
        "math_symbols": "",
        "title": "Attention is All you Need",
        "type": "Conference Proceedings",
        "has_math": false,
        "page_label": "6",
        "id": "/home/ankur/Downloads/iiit/rag/pdfs/NIPS-2017-attention-is-all-you-need-Paper.pdf:5:0",
        "description-abstract": "The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.",
        "eventtype": "Poster",
        "creator": "PyPDF",
        "editors": "I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett",
        "canonical_forms": "",
        "description": "Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)",
        "publisher": "Curran Associates, Inc.",
        "lastpage": "6008",
        "source": "/home/ankur/Downloads/iiit/rag/pdfs/NIPS-2017-attention-is-all-you-need-Paper.pdf",
        "math_density": 0.0
      },
      "feedback": "accepted",
      "timestamp": "2025-11-18T23:15:59.522306"
    },
    {
      "bullet_id": "bullet_9",
      "bullet_text": "V) 2. Multi-Head Attention = Concat(head1,..., headh)WO, where headi = Attention(QWQ_i, KW_K_i, VW_V_i)",
      "source_chunk": "MultiHead(Q,K,V ) = Concat(head1,..., headh)WO\nwhere headi = Attention(QWQ\ni ,KW K\ni ,VW V\ni )\nWhere the projections are parameter matricesWQ\ni ∈Rdmodel×dk , WK\ni ∈Rdmodel×dk , WV\ni ∈Rdmodel×dv\nand WO ∈Rhdv×dmodel .\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\ndk = dv = dmodel/h= 64. Due to the reduced dimension of each head, the total computational cost\nis similar to that of single-head attention with full dimensionality.\n3.2.3 Applications of Attention in our Model\nThe Transformer uses multi-head attention in three different ways:\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\nand the memory keys and values come from the output of the encoder. This allows every",
      "source_metadata": {
        "editors": "I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett",
        "page_label": "5",
        "creator": "PyPDF",
        "total_pages": 11,
        "author": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin",
        "canonical_forms": "",
        "id": "/home/ankur/Downloads/iiit/rag/pdfs/NIPS-2017-attention-is-all-you-need-Paper.pdf:4:0",
        "creationdate": "",
        "math_symbols": "",
        "subject": "Neural Information Processing Systems http://nips.cc/",
        "book": "Advances in Neural Information Processing Systems 30",
        "equation_count": 0,
        "sympy_parsed": false,
        "description": "Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)",
        "firstpage": "5998",
        "type": "Conference Proceedings",
        "has_math": false,
        "source": "/home/ankur/Downloads/iiit/rag/pdfs/NIPS-2017-attention-is-all-you-need-Paper.pdf",
        "lastpage": "6008",
        "publisher": "Curran Associates, Inc.",
        "language": "en-US",
        "title": "Attention is All you Need",
        "created": "2017",
        "producer": "PyPDF2",
        "equation_ids": "",
        "page": 4,
        "moddate": "2018-02-12T21:22:10-08:00",
        "published": "2017",
        "date": "2017",
        "math_density": 0.0,
        "eventtype": "Poster",
        "description-abstract": "The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1."
      },
      "feedback": "accepted",
      "timestamp": "2025-11-18T23:16:04.382919"
    },
    {
      "bullet_id": "bullet_8",
      "bullet_text": "**Interpretable Models**: Self-attention could yield more interpretable models, as individual attention heads learn to perform different tasks. This is demonstrated by inspecting attention distributions from the models and presenting examples in the appendix.",
      "source_chunk": "convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\nthe approach we take in our model.\nAs side beneﬁt, self-attention could yield more interpretable models. We inspect attention distributions\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\nand semantic structure of the sentences.\n5 Training\nThis section describes the training regime for our models.\n5.1 Training Data and Batching\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-",
      "source_metadata": {
        "publisher": "Curran Associates, Inc.",
        "description": "Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)",
        "math_density": 0.0,
        "date": "2017",
        "page_label": "7",
        "producer": "PyPDF2",
        "has_math": false,
        "moddate": "2018-02-12T21:22:10-08:00",
        "equation_ids": "",
        "firstpage": "5998",
        "type": "Conference Proceedings",
        "source": "/home/ankur/Downloads/iiit/rag/pdfs/NIPS-2017-attention-is-all-you-need-Paper.pdf",
        "published": "2017",
        "total_pages": 11,
        "subject": "Neural Information Processing Systems http://nips.cc/",
        "lastpage": "6008",
        "created": "2017",
        "eventtype": "Poster",
        "title": "Attention is All you Need",
        "sympy_parsed": false,
        "math_symbols": "",
        "book": "Advances in Neural Information Processing Systems 30",
        "language": "en-US",
        "page": 6,
        "editors": "I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett",
        "description-abstract": "The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.",
        "author": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin",
        "id": "/home/ankur/Downloads/iiit/rag/pdfs/NIPS-2017-attention-is-all-you-need-Paper.pdf:6:1",
        "creator": "PyPDF",
        "canonical_forms": "",
        "creationdate": "",
        "equation_count": 0
      },
      "feedback": "accepted",
      "timestamp": "2025-11-18T23:16:05.949926"
    },
    {
      "bullet_id": "bullet_3",
      "bullet_text": "Speaker Notes: 1. \"Ladies and gentlemen, today we're going to talk about a breakthrough in natural language processing that's been making waves in the industry. Introducing Self Attention, a game-changing technique that's revolutionizing the way we process human language.\" 2. \"Self Attention is a type of attention mechanism that allows models to focus on different parts of an input sequence simultaneously, enabling them to capture complex relationships between words and phrases.\" 3. \"This breakthrough has significant implications for areas like machine translation, question answering, and text summarization, making Self Attention a must-know for anyone interested in NLP.\"",
      "source_chunk": "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations [4, 22, 23, 19].\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\naligned recurrence and have been shown to perform well on simple-language question answering and\nlanguage modeling tasks [28].",
      "source_metadata": {
        "published": "2017",
        "producer": "PyPDF2",
        "math_symbols": "",
        "eventtype": "Poster",
        "editors": "I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett",
        "page": 1,
        "creationdate": "",
        "author": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin",
        "title": "Attention is All you Need",
        "math_density": 0.0,
        "creator": "PyPDF",
        "description-abstract": "The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.",
        "moddate": "2018-02-12T21:22:10-08:00",
        "equation_ids": "",
        "lastpage": "6008",
        "publisher": "Curran Associates, Inc.",
        "subject": "Neural Information Processing Systems http://nips.cc/",
        "page_label": "2",
        "total_pages": 11,
        "firstpage": "5998",
        "type": "Conference Proceedings",
        "book": "Advances in Neural Information Processing Systems 30",
        "description": "Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)",
        "created": "2017",
        "date": "2017",
        "source": "/home/ankur/Downloads/iiit/rag/pdfs/NIPS-2017-attention-is-all-you-need-Paper.pdf",
        "equation_count": 0,
        "canonical_forms": "",
        "id": "/home/ankur/Downloads/iiit/rag/pdfs/NIPS-2017-attention-is-all-you-need-Paper.pdf:1:3",
        "has_math": false,
        "sympy_parsed": false,
        "language": "en-US"
      },
      "feedback": "accepted",
      "timestamp": "2025-11-18T23:31:28.795879"
    },
    {
      "bullet_id": "bullet_1",
      "bullet_text": "Subtitle: \"A Game-Changer for Natural Language Processing\"",
      "source_chunk": "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations [4, 22, 23, 19].\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\naligned recurrence and have been shown to perform well on simple-language question answering and\nlanguage modeling tasks [28].",
      "source_metadata": {
        "published": "2017",
        "producer": "PyPDF2",
        "math_symbols": "",
        "eventtype": "Poster",
        "editors": "I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett",
        "page": 1,
        "creationdate": "",
        "author": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin",
        "title": "Attention is All you Need",
        "math_density": 0.0,
        "creator": "PyPDF",
        "description-abstract": "The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.",
        "moddate": "2018-02-12T21:22:10-08:00",
        "equation_ids": "",
        "lastpage": "6008",
        "publisher": "Curran Associates, Inc.",
        "subject": "Neural Information Processing Systems http://nips.cc/",
        "page_label": "2",
        "total_pages": 11,
        "firstpage": "5998",
        "type": "Conference Proceedings",
        "book": "Advances in Neural Information Processing Systems 30",
        "description": "Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)",
        "created": "2017",
        "date": "2017",
        "source": "/home/ankur/Downloads/iiit/rag/pdfs/NIPS-2017-attention-is-all-you-need-Paper.pdf",
        "equation_count": 0,
        "canonical_forms": "",
        "id": "/home/ankur/Downloads/iiit/rag/pdfs/NIPS-2017-attention-is-all-you-need-Paper.pdf:1:3",
        "has_math": false,
        "sympy_parsed": false,
        "language": "en-US"
      },
      "feedback": "accepted",
      "timestamp": "2025-11-18T23:31:33.396883"
    },
    {
      "bullet_id": "bullet_2",
      "bullet_text": "Image: a diagram illustrating self attention mechanism",
      "source_chunk": "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations [4, 22, 23, 19].\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\naligned recurrence and have been shown to perform well on simple-language question answering and\nlanguage modeling tasks [28].",
      "source_metadata": {
        "published": "2017",
        "producer": "PyPDF2",
        "math_symbols": "",
        "eventtype": "Poster",
        "editors": "I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett",
        "page": 1,
        "creationdate": "",
        "author": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin",
        "title": "Attention is All you Need",
        "math_density": 0.0,
        "creator": "PyPDF",
        "description-abstract": "The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.",
        "moddate": "2018-02-12T21:22:10-08:00",
        "equation_ids": "",
        "lastpage": "6008",
        "publisher": "Curran Associates, Inc.",
        "subject": "Neural Information Processing Systems http://nips.cc/",
        "page_label": "2",
        "total_pages": 11,
        "firstpage": "5998",
        "type": "Conference Proceedings",
        "book": "Advances in Neural Information Processing Systems 30",
        "description": "Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)",
        "created": "2017",
        "date": "2017",
        "source": "/home/ankur/Downloads/iiit/rag/pdfs/NIPS-2017-attention-is-all-you-need-Paper.pdf",
        "equation_count": 0,
        "canonical_forms": "",
        "id": "/home/ankur/Downloads/iiit/rag/pdfs/NIPS-2017-attention-is-all-you-need-Paper.pdf:1:3",
        "has_math": false,
        "sympy_parsed": false,
        "language": "en-US"
      },
      "feedback": "accepted",
      "timestamp": "2025-11-18T23:31:35.029538"
    },
    {
      "bullet_id": "bullet_5",
      "bullet_text": "Subtitle: \"A Closer Look at the Scaled Dot-Product Attention Mechanism\"",
      "source_chunk": "Scaled Dot-Product Attention\n Multi-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are also packed together into matrices Kand V. We compute\nthe matrix of outputs as:\nAttention(Q,K,V ) = softmax(QKT\n√dk\n)V (1)\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\nof 1√dk",
      "source_metadata": {
        "description": "Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)",
        "publisher": "Curran Associates, Inc.",
        "equation_count": 0,
        "total_pages": 11,
        "page_label": "4",
        "producer": "PyPDF2",
        "page": 3,
        "math_density": 0.0,
        "has_math": false,
        "firstpage": "5998",
        "date": "2017",
        "math_symbols": "",
        "eventtype": "Poster",
        "book": "Advances in Neural Information Processing Systems 30",
        "creationdate": "",
        "title": "Attention is All you Need",
        "subject": "Neural Information Processing Systems http://nips.cc/",
        "language": "en-US",
        "moddate": "2018-02-12T21:22:10-08:00",
        "author": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin",
        "description-abstract": "The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.",
        "lastpage": "6008",
        "creator": "PyPDF",
        "sympy_parsed": false,
        "canonical_forms": "",
        "source": "/home/ankur/Downloads/iiit/rag/pdfs/NIPS-2017-attention-is-all-you-need-Paper.pdf",
        "editors": "I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett",
        "published": "2017",
        "created": "2017",
        "equation_ids": "",
        "type": "Conference Proceedings",
        "id": "/home/ankur/Downloads/iiit/rag/pdfs/NIPS-2017-attention-is-all-you-need-Paper.pdf:3:0"
      },
      "feedback": "rejected",
      "timestamp": "2025-11-18T23:31:41.676623"
    }
  ]
}